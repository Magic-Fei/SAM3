# SAM3 æ¨¡å‹å‹ç¼©æ–¹æ¡ˆ

SAM3 å®Œæ•´æ¨¡å‹çº¦ **2-3 GB**ï¼Œä¸»è¦ç”±ä»¥ä¸‹éƒ¨åˆ†ç»„æˆï¼š
- **è§†è§‰ç¼–ç å™¨ï¼ˆViTï¼‰**: ~2 GB ï¼ˆæœ€å¤§ç»„ä»¶ï¼‰
- **æ–‡æœ¬ç¼–ç å™¨**: ~500 MB
- **Transformer + åˆ†å‰²å¤´**: ~300 MB

---

## æ–¹æ¡ˆ 1: ä»…ä¿å­˜å¿…è¦çš„æƒé‡ï¼ˆæ¨èï¼Œæœ€ç®€å•ï¼‰â­

è®­ç»ƒæ—¶ checkpoint åŒ…å«å¾ˆå¤šè®­ç»ƒç›¸å…³çš„çŠ¶æ€ï¼Œæ¨ç†æ—¶ä¸éœ€è¦ã€‚

### æ“ä½œæ­¥éª¤

**æ–¹æ³• Aï¼šè®­ç»ƒå®Œæˆåæå–æ¨¡å‹æƒé‡**

åˆ›å»ºè„šæœ¬ `tools/extract_model_weights.py`ï¼š

```python
import torch
from pathlib import Path

def extract_model_only(checkpoint_path, output_path):
    """ä»å®Œæ•´ checkpoint æå–çº¯æ¨¡å‹æƒé‡"""
    print(f"Loading checkpoint from: {checkpoint_path}")
    checkpoint = torch.load(checkpoint_path, map_location="cpu")
    
    # æå–æ¨¡å‹æƒé‡
    if "model" in checkpoint:
        model_state = checkpoint["model"]
    else:
        model_state = checkpoint
    
    # ä¿å­˜çº¯æƒé‡
    torch.save(model_state, output_path)
    
    # æ˜¾ç¤ºå¤§å°å¯¹æ¯”
    original_size = Path(checkpoint_path).stat().st_size / (1024**2)
    new_size = Path(output_path).stat().st_size / (1024**2)
    
    print(f"åŸå§‹ checkpoint: {original_size:.1f} MB")
    print(f"çº¯æ¨¡å‹æƒé‡: {new_size:.1f} MB")
    print(f"èŠ‚çœç©ºé—´: {original_size - new_size:.1f} MB")

if __name__ == "__main__":
    extract_model_only(
        checkpoint_path="experiments/checkpoints/checkpoint.pt",
        output_path="experiments/model_weights_only.pt"
    )
```

è¿è¡Œï¼š
```bash
python tools/extract_model_weights.py
```

**èŠ‚çœç©ºé—´ï¼šçº¦ 20-30%**ï¼ˆå»æ‰ optimizerã€loss ç­‰çŠ¶æ€ï¼‰

---

## æ–¹æ¡ˆ 2: FP16 åŠç²¾åº¦é‡åŒ–ï¼ˆæ¨èï¼‰â­â­

å°†æ¨¡å‹ä» FP32 è½¬æ¢ä¸º FP16ï¼Œ**å‡å° 50% å­˜å‚¨ç©ºé—´**ï¼Œç²¾åº¦æŸå¤±å¾ˆå°ã€‚

### æ“ä½œæ­¥éª¤

åˆ›å»ºè„šæœ¬ `tools/quantize_to_fp16.py`ï¼š

```python
import torch
from pathlib import Path

def quantize_to_fp16(checkpoint_path, output_path):
    """å°†æ¨¡å‹é‡åŒ–ä¸º FP16"""
    print(f"Loading checkpoint...")
    checkpoint = torch.load(checkpoint_path, map_location="cpu")
    
    if "model" in checkpoint:
        model_state = checkpoint["model"]
    else:
        model_state = checkpoint
    
    # è½¬æ¢ä¸º FP16
    print("Converting to FP16...")
    model_state_fp16 = {
        k: v.half() if v.dtype == torch.float32 else v
        for k, v in model_state.items()
    }
    
    torch.save(model_state_fp16, output_path)
    
    original_size = Path(checkpoint_path).stat().st_size / (1024**2)
    new_size = Path(output_path).stat().st_size / (1024**2)
    
    print(f"âœ“ FP32 æ¨¡å‹: {original_size:.1f} MB")
    print(f"âœ“ FP16 æ¨¡å‹: {new_size:.1f} MB")
    print(f"âœ“ èŠ‚çœ: {original_size - new_size:.1f} MB ({(1-new_size/original_size)*100:.1f}%)")

if __name__ == "__main__":
    quantize_to_fp16(
        checkpoint_path="experiments/checkpoints/checkpoint.pt",
        output_path="experiments/model_fp16.pt"
    )
```

æ¨ç†æ—¶åŠ è½½ FP16 æ¨¡å‹ï¼š
```python
model.load_state_dict(torch.load("model_fp16.pt"), strict=False)
model = model.half()  # ç¡®ä¿æ¨¡å‹ä½¿ç”¨ FP16
```

**ä¼˜ç‚¹**ï¼š
- âœ… æ¨¡å‹å¤§å°å‡åŠ
- âœ… æ¨ç†é€Ÿåº¦å¯èƒ½æ›´å¿«ï¼ˆGPU æ”¯æŒ FP16ï¼‰
- âœ… ç²¾åº¦æŸå¤±æå°ï¼ˆé€šå¸¸ <1%ï¼‰

---

## æ–¹æ¡ˆ 3: ä¿®æ”¹æ¨¡å‹æ¶æ„ï¼ˆéœ€è¦é‡æ–°è®­ç»ƒï¼‰

å¦‚æœéœ€è¦æ›´å°çš„æ¨¡å‹ï¼Œå¯ä»¥ä¿®æ”¹æ¶æ„åé‡æ–°è®­ç»ƒã€‚

### 3.1 å‡å° ViT æ·±åº¦å’Œå®½åº¦

ä¿®æ”¹ `sam3/model_builder.py` ä¸­çš„ `_create_vit_backbone`ï¼š

```python
def _create_vit_backbone(compile_mode=None):
    """è½»é‡çº§ ViT"""
    return ViT(
        img_size=1008,
        patch_size=14,
        embed_dim=768,      # åŸ: 1024 â†’ å‡å°‘ 25%
        depth=24,           # åŸ: 32   â†’ å‡å°‘ 25%
        num_heads=12,       # åŸ: 16   â†’ å‡å°‘ 25%
        mlp_ratio=4.0,
        norm_layer="LayerNorm",
        drop_path_rate=0.1,
        compile_mode=compile_mode,
    )
```

**é¢„æœŸæ•ˆæœ**ï¼š
- æ¨¡å‹å¤§å°ï¼š~1.5 GBï¼ˆå‡å°‘ ~40%ï¼‰
- æ€§èƒ½æŸå¤±ï¼š5-10% mAP

### 3.2 å‡å° Transformer å±‚æ•°

ä¿®æ”¹ `_create_transformer_encoder`ï¼š

```python
encoder = TransformerEncoderFusion(
    layer=encoder_layer,
    num_layers=4,        # åŸ: 6 â†’ å‡å°‘ 33%
    d_model=256,
    ...
)
```

ä¿®æ”¹ `_create_transformer_decoder`ï¼š

```python
decoder = TransformerDecoder(
    decoder_layer=decoder_layer,
    num_layers=4,        # åŸ: 6 â†’ å‡å°‘ 33%
    ...
)
```

**é¢„æœŸæ•ˆæœ**ï¼š
- é¢å¤–å‡å°‘ï¼š~200 MB
- æ€§èƒ½æŸå¤±ï¼š3-5% mAP

---

## æ–¹æ¡ˆ 4: INT8 é‡åŒ–ï¼ˆæœ€æ¿€è¿›ï¼‰

ä½¿ç”¨ PyTorch çš„ INT8 é‡åŒ–ï¼Œå¯å°†æ¨¡å‹å‹ç¼©åˆ° **25% åŸå§‹å¤§å°**ã€‚

### æ“ä½œæ­¥éª¤

```python
import torch
from torch.quantization import quantize_dynamic

# åŠ è½½æ¨¡å‹
model = build_sam3_image_model(...)

# åŠ¨æ€é‡åŒ–ï¼ˆä»…é‡åŒ–æƒé‡ï¼‰
model_quantized = quantize_dynamic(
    model, 
    {torch.nn.Linear, torch.nn.Conv2d},  # é‡åŒ–çš„å±‚ç±»å‹
    dtype=torch.qint8
)

# ä¿å­˜é‡åŒ–æ¨¡å‹
torch.save(model_quantized.state_dict(), "model_int8.pt")
```

**ä¼˜ç‚¹**ï¼š
- âœ… æ¨¡å‹æœ€å°ï¼ˆ~500 MBï¼‰
- âœ… CPU æ¨ç†æ›´å¿«

**ç¼ºç‚¹**ï¼š
- âŒ ç²¾åº¦æŸå¤±è¾ƒå¤§ï¼ˆ5-15% mAPï¼‰
- âŒ GPU åŠ é€Ÿæ•ˆæœæœ‰é™

---

## æ¨èæ–¹æ¡ˆç»„åˆ ğŸ¯

### æœ€ä½³å®è·µï¼šæ–¹æ¡ˆ 1 + æ–¹æ¡ˆ 2

```bash
# æ­¥éª¤ 1: æå–çº¯æ¨¡å‹æƒé‡
python tools/extract_model_weights.py

# æ­¥éª¤ 2: è½¬æ¢ä¸º FP16
python tools/quantize_to_fp16.py
```

**æœ€ç»ˆæ•ˆæœ**ï¼š
- åŸå§‹ checkpoint: ~2.5 GB
- çº¯æƒé‡ FP32: ~2.0 GB
- çº¯æƒé‡ FP16: ~1.0 GB âœ¨
- **æ€»å‹ç¼©ç‡ï¼š60%ï¼Œå‡ ä¹æ— ç²¾åº¦æŸå¤±**

---

## ä½¿ç”¨å‹ç¼©åçš„æ¨¡å‹

ä¿®æ”¹ `tools/batch_inference.py` ä¸­çš„åŠ è½½ä»£ç ï¼š

```python
def load_model(ckpt_path: Path, device: torch.device) -> Sam3Processor:
    model = build_sam3_image_model(...)
    
    state_dict = torch.load(ckpt_path, map_location=device)
    model.load_state_dict(state_dict, strict=False)
    
    # å¦‚æœæ˜¯ FP16 æ¨¡å‹
    if device.type == "cuda":
        model = model.half()  # ä½¿ç”¨ FP16
    
    model.eval()
    processor = Sam3Processor(model, device=str(device))
    return processor
```

---

## å¯¹æ¯”è¡¨æ ¼

| æ–¹æ¡ˆ | æ¨¡å‹å¤§å° | å‹ç¼©ç‡ | ç²¾åº¦æŸå¤± | éœ€é‡è®­ç»ƒ | æ¨èåº¦ |
|------|---------|--------|---------|---------|--------|
| åŸå§‹æ¨¡å‹ | 2.5 GB | - | 0% | - | - |
| æå–æƒé‡ | 2.0 GB | 20% | 0% | âŒ | â­â­â­ |
| FP16 é‡åŒ– | 1.0 GB | 60% | <1% | âŒ | â­â­â­â­â­ |
| å‡å°æ¶æ„ | 1.5 GB | 40% | 5-10% | âœ… | â­â­â­ |
| INT8 é‡åŒ– | 0.5 GB | 80% | 10-15% | âŒ | â­â­ |

---

## å¿«é€Ÿå¼€å§‹

æˆ‘å¯ä»¥ç«‹å³å¸®ä½ åˆ›å»ºè¿™ä¸¤ä¸ªè„šæœ¬ï¼š
1. `tools/extract_model_weights.py` - æå–çº¯æƒé‡
2. `tools/quantize_to_fp16.py` - FP16 é‡åŒ–

åªéœ€è¦è¿è¡Œè¿™ä¸¤ä¸ªè„šæœ¬ï¼Œå°±èƒ½å°†æ¨¡å‹ä» 2.5 GB å‹ç¼©åˆ° 1.0 GBï¼Œè€Œä¸”å‡ ä¹æ²¡æœ‰ç²¾åº¦æŸå¤±ï¼

è¦æˆ‘ç°åœ¨åˆ›å»ºè¿™äº›è„šæœ¬å—ï¼Ÿ

