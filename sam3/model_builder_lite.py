"""
è½»é‡çº§ SAM3 æ¨¡å‹æ„å»ºå™¨

ç›¸æ¯”æ ‡å‡† SAM3ï¼š
- ViT: embed_dim 1024â†’768, depth 32â†’24 (å‡å°‘ 40%)
- Transformer: layers 6â†’4 (å‡å°‘ 33%)
- æœ€ç»ˆæ¨¡å‹: ~1.5 GB (æ ‡å‡†ç‰ˆ ~2.5 GB)
- æ€§èƒ½æŸå¤±: é¢„è®¡ 5-10% mAP
"""

import os
from typing import Optional

import torch
import torch.nn as nn

from sam3.model.sam3_image import Sam3Image
from sam3.model.vl_combiner import SAM3VLBackbone
from sam3.model.necks import Sam3DualViTDetNeck
from sam3.model.model_misc import TransformerWrapper, DotProductScoring, MLP, MultiheadAttentionWrapper as MultiheadAttention
from sam3.model.position_encoding import PositionEmbeddingSine
from sam3.model.vitdet import ViT
from sam3.model.text_encoder_ve import VETextEncoder
from sam3.model.tokenizer_ve import SimpleTokenizer
from sam3.model.encoder import TransformerEncoderFusion, TransformerEncoderLayer
from sam3.model.decoder import TransformerDecoder, TransformerDecoderLayer
from sam3.model.geometry_encoders import SequenceGeometryEncoder
from sam3.model.maskformer_segmentation import UniversalSegmentationHead, PixelDecoder
from sam3.model.memory import CXBlock
from sam3.model_builder import (
    _load_checkpoint,
    _setup_device_and_mode,
    download_ckpt_from_hf,
)


def _create_vit_backbone_lite(compile_mode=None):
    """åˆ›å»ºè½»é‡çº§ ViT backbone (å‡å°‘ 40%)"""
    return ViT(
        img_size=1008,
        pretrain_img_size=336,
        patch_size=14,
        in_chans=3,
        embed_dim=768,  # åŸ: 1024
        depth=24,  # åŸ: 32
        num_heads=12,  # åŸ: 16
        mlp_ratio=4.0,  # åŸ: 4.625
        norm_layer="LayerNorm",
        drop_path_rate=0.1,
        qkv_bias=True,
        use_abs_pos=True,
        tile_abs_pos=True,
        global_att_blocks=(5, 11, 17, 23),
        rel_pos_blocks=(),
        use_rope=True,
        use_interp_rope=True,
        window_size=24,
        pretrain_use_cls_token=True,
        retain_cls_token=False,
        ln_pre=True,
        ln_post=False,
        return_interm_layers=False,
        bias_patch_embed=False,
        compile_mode=compile_mode,
    )


def _create_transformer_encoder_lite():
    """åˆ›å»ºè½»é‡çº§ Transformer encoder (å‡å°‘ 33%)"""
    encoder_layer = TransformerEncoderLayer(
        activation="relu",
        d_model=256,
        dim_feedforward=2048,
        dropout=0.1,
        pos_enc_at_attn=True,
        pos_enc_at_cross_attn_keys=False,
        pos_enc_at_cross_attn_queries=False,
        pre_norm=True,
        self_attention=MultiheadAttention(
            num_heads=8,
            dropout=0.1,
            embed_dim=256,
            batch_first=True,
        ),
        cross_attention=MultiheadAttention(
            num_heads=8,
            dropout=0.1,
            embed_dim=256,
            batch_first=True,
        ),
    )

    encoder = TransformerEncoderFusion(
        layer=encoder_layer,
        num_layers=4,       # åŸ: 6
        d_model=256,
        num_feature_levels=1,
        frozen=False,
        use_act_checkpoint=True,
        add_pooled_text_to_img_feat=False,
        pool_text_with_mask=True,
    )
    return encoder


def _create_transformer_decoder_lite():
    """åˆ›å»ºè½»é‡çº§ Transformer decoder (å‡å°‘ 33%)"""
    decoder_layer = TransformerDecoderLayer(
        activation="relu",
        d_model=256,
        dim_feedforward=2048,
        dropout=0.1,
        cross_attention=MultiheadAttention(
            num_heads=8,
            dropout=0.1,
            embed_dim=256,
        ),
        n_heads=8,
        use_text_cross_attention=True,
    )

    decoder = TransformerDecoder(
        layer=decoder_layer,
        num_layers=4,  # åŸ: 6
        num_queries=200,
        return_intermediate=True,
        box_refine=True,
        num_o2m_queries=0,
        dac=True,
        boxRPB="log",
        d_model=256,
        frozen=False,
        interaction_layer=None,
        dac_use_selfatt_ln=True,
        resolution=1008,
        stride=14,
        use_act_checkpoint=True,
        presence_token=True,
    )
    return decoder


def _create_geometry_encoder_lite():
    """åˆ›å»ºå‡ ä½•ç¼–ç å™¨ (ä¿æŒåŸå¤§å°ï¼Œå› ä¸ºç›¸å¯¹è¾ƒå°)"""
    # Position encoding for geometry encoder - must match standard config
    geo_pos_enc = PositionEmbeddingSine(
        num_pos_feats=256,  # Same as standard model
        normalize=True,
        scale=None,
        temperature=10000,
    )
    
    # Create geometry encoder layer
    geo_layer = TransformerEncoderLayer(
        activation="relu",
        d_model=256,
        dim_feedforward=2048,
        dropout=0.1,
        pos_enc_at_attn=False,
        pre_norm=True,
        self_attention=MultiheadAttention(
            num_heads=8,
            dropout=0.1,
            embed_dim=256,
            batch_first=False,
        ),
        pos_enc_at_cross_attn_queries=False,
        pos_enc_at_cross_attn_keys=True,
        cross_attention=MultiheadAttention(
            num_heads=8,
            dropout=0.1,
            embed_dim=256,
            batch_first=False,
        ),
    )

    # Create geometry encoder
    input_geometry_encoder = SequenceGeometryEncoder(
        pos_enc=geo_pos_enc,
        encode_boxes_as_points=False,
        points_direct_project=True,
        points_pool=True,
        points_pos_enc=True,
        boxes_direct_project=True,
        boxes_pool=True,
        boxes_pos_enc=True,
        d_model=256,
        num_layers=3,
        layer=geo_layer,
        use_act_ckpt=True,
        add_cls=True,
        add_post_encode_proj=True,
    )
    return input_geometry_encoder


def _create_segmentation_head_lite(compile_mode=None):
    """åˆ›å»ºåˆ†å‰²å¤´ (ä¿æŒåŸå¤§å°ï¼Œå› ä¸ºç›¸å¯¹è¾ƒå°)"""
    pixel_decoder = PixelDecoder(
        num_upsampling_stages=3,
        interpolation_mode="nearest",
        hidden_dim=256,
        compile_mode=compile_mode,
    )

    cross_attend_prompt = MultiheadAttention(
        num_heads=8,
        dropout=0,
        embed_dim=256,
    )

    segmentation_head = UniversalSegmentationHead(
        hidden_dim=256,
        upsampling_stages=3,
        aux_masks=False,
        presence_head=False,
        dot_product_scorer=None,
        act_ckpt=True,
        cross_attend_prompt=cross_attend_prompt,
        pixel_decoder=pixel_decoder,
    )
    return segmentation_head


def build_sam3_lite_model(
    bpe_path=None,
    device="cuda" if torch.cuda.is_available() else "cpu",
    eval_mode=True,
    checkpoint_path=None,
    load_from_HF=False,
    enable_segmentation=True,
    compile=False,
):
    """
    æ„å»ºè½»é‡çº§ SAM3 å›¾åƒæ¨¡å‹
    
    ç›¸æ¯”æ ‡å‡†ç‰ˆæœ¬ï¼š
    - ViT å‡å°‘ 40% (768 dim, 24 layers)
    - Transformer å‡å°‘ 33% (4 layers)
    - æ¨¡å‹å¤§å°: ~1.5 GB (vs 2.5 GB)
    - æ¨ç†é€Ÿåº¦: å¿« 20-30%
    - æ€§èƒ½: é¢„è®¡æŸå¤± 5-10% mAP
    
    Args:
        bpe_path: BPE tokenizer è·¯å¾„
        device: è®¾å¤‡ ('cuda' æˆ– 'cpu')
        eval_mode: æ˜¯å¦è¯„ä¼°æ¨¡å¼
        checkpoint_path: æƒé‡è·¯å¾„
        load_from_HF: æ˜¯å¦ä» HuggingFace ä¸‹è½½
        enable_segmentation: æ˜¯å¦å¯ç”¨åˆ†å‰²å¤´
        compile: æ˜¯å¦ç¼–è¯‘æ¨¡å‹
    
    Returns:
        è½»é‡çº§ SAM3 å›¾åƒæ¨¡å‹
    """
    if bpe_path is None:
        bpe_path = os.path.join(
            os.path.dirname(__file__), "..", "assets", "bpe_simple_vocab_16e6.txt.gz"
        )
    
    print("ğŸš€ æ„å»ºè½»é‡çº§ SAM3 æ¨¡å‹...")
    print("   - ViT: 768 dim, 24 layers (å‡å°‘ 40%)")
    print("   - Transformer: 4 layers (å‡å°‘ 33%)")
    print("   - é¢„è®¡æ¨¡å‹å¤§å°: ~1.5 GB")
    
    # åˆ›å»ºè½»é‡çº§ç»„ä»¶
    compile_mode = "default" if compile else None
    
    # è½»é‡çº§ ViT
    vision_encoder = _create_vit_backbone_lite(compile_mode=compile_mode)
    
    # Position encoding (for visual backbone/neck)
    position_encoding = PositionEmbeddingSine(
        num_pos_feats=256,  # Same as standard model
        normalize=True,
        scale=None,
        temperature=10000,
    )
    
    # ViT neck
    vit_neck = Sam3DualViTDetNeck(
        position_encoding=position_encoding,
        d_model=256,
        scale_factors=[4.0, 2.0, 1.0, 0.5],
        trunk=vision_encoder,
        add_sam2_neck=False,
    )
    
    # Text encoder (ä¿æŒåŸå¤§å°ï¼Œå› ä¸ºç›¸å¯¹è¾ƒå°)
    tokenizer = SimpleTokenizer(bpe_path=bpe_path)
    text_encoder = VETextEncoder(
        tokenizer=tokenizer,
        d_model=256,
        width=1024,
        heads=16,
        layers=24,
    )
    
    # VL Backbone
    backbone = SAM3VLBackbone(visual=vit_neck, text=text_encoder, scalp=1)
    
    # è½»é‡çº§ Transformer
    encoder = _create_transformer_encoder_lite()
    decoder = _create_transformer_decoder_lite()
    transformer = TransformerWrapper(encoder=encoder, decoder=decoder, d_model=256)
    
    # Geometry encoder
    input_geometry_encoder = _create_geometry_encoder_lite()
    
    # Segmentation head
    if enable_segmentation:
        segmentation_head = _create_segmentation_head_lite(compile_mode=compile_mode)
    else:
        segmentation_head = None
    
    # Dot product scoring
    dot_prod_mlp = MLP(
        input_dim=256,
        hidden_dim=2048,
        output_dim=256,
        num_layers=2,
        dropout=0.1,
        residual=True,
        out_norm=nn.LayerNorm(256),
    )
    dot_prod_scoring = DotProductScoring(
        d_model=256,
        d_proj=256,
        prompt_mlp=dot_prod_mlp,
    )
    
    # Matcher (è®­ç»ƒæ—¶éœ€è¦)
    matcher = None
    if not eval_mode:
        from sam3.train.matcher import BinaryHungarianMatcherV2
        matcher = BinaryHungarianMatcherV2(
            focal=True,
            cost_class=2.0,
            cost_bbox=5.0,
            cost_giou=2.0,
            alpha=0.25,
            gamma=2,
            stable=False,
        )
    
    # åˆ›å»ºæ¨¡å‹
    model = Sam3Image(
        backbone=backbone,
        transformer=transformer,
        input_geometry_encoder=input_geometry_encoder,
        segmentation_head=segmentation_head,
        num_feature_levels=1,
        o2m_mask_predict=True,
        dot_prod_scoring=dot_prod_scoring,
        use_instance_query=False,
        multimask_output=True,
        inst_interactive_predictor=None,
        matcher=matcher,
        use_dot_prod_scoring=True,
    )
    
    # åŠ è½½æƒé‡
    if load_from_HF and checkpoint_path is None:
        checkpoint_path = download_ckpt_from_hf()
    if checkpoint_path is not None:
        print(f"ğŸ“‚ åŠ è½½æƒé‡: {checkpoint_path}")
        _load_checkpoint(model, checkpoint_path)
    
    # è®¾ç½®è®¾å¤‡å’Œæ¨¡å¼
    model = _setup_device_and_mode(model, device, eval_mode)
    
    # ç»Ÿè®¡å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    print(f"âœ“ æ¨¡å‹å‚æ•°é‡: {total_params/1e6:.1f}M (æ ‡å‡†ç‰ˆ ~600M)")
    
    return model

