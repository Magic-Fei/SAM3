#!/usr/bin/env python
"""
å°†æ¨¡å‹é‡åŒ–ä¸º FP16 åŠç²¾åº¦ï¼Œå‡å° 50% å­˜å‚¨ç©ºé—´ï¼Œç²¾åº¦æŸå¤±æå°ï¼ˆ<1%ï¼‰ã€‚

ä½¿ç”¨æ–¹æ³•:
    python tools/quantize_to_fp16.py
"""

import torch
from pathlib import Path

# ============================================================================
# é…ç½®
# ============================================================================
INPUT_PATH = Path(r"D:\qianpf\code\sam3-main\experiments_guajia\checkpoints\checkpoint_10.pt")  # æˆ–ç›´æ¥ç”¨ checkpoint.pt
OUTPUT_PATH = Path(r"D:\qianpf\code\sam3-main\experiments_guajia\checkpoints\checkpoint_fp16.pt")


def quantize_to_fp16(input_path: Path, output_path: Path):
    """å°†æ¨¡å‹é‡åŒ–ä¸º FP16"""
    print("=" * 70)
    print("FP16 åŠç²¾åº¦é‡åŒ–å·¥å…·")
    print("=" * 70)
    
    if not input_path.exists():
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
        print(f"\næç¤º: å¯ä»¥å…ˆè¿è¡Œ extract_model_weights.py æå–çº¯æƒé‡")
        print(f"      æˆ–ç›´æ¥ä½¿ç”¨å®Œæ•´çš„ checkpoint.pt")
        return
    
    print(f"\nğŸ“‚ åŠ è½½æ¨¡å‹: {input_path}")
    checkpoint = torch.load(input_path, map_location="cpu")
    
    # æå–æ¨¡å‹æƒé‡
    if "model" in checkpoint:
        model_state = checkpoint["model"]
        print(f"âœ“ æ‰¾åˆ°æ¨¡å‹æƒé‡ (å®Œæ•´ checkpoint)")
    elif "state_dict" in checkpoint:
        model_state = checkpoint["state_dict"]
        print(f"âœ“ æ‰¾åˆ°æ¨¡å‹æƒé‡ (state_dict)")
    else:
        model_state = checkpoint
        print(f"âœ“ ä½¿ç”¨æ•´ä¸ªæ–‡ä»¶ä½œä¸ºæ¨¡å‹æƒé‡")
    
    # ç»Ÿè®¡å‚æ•°ä¿¡æ¯
    total_params = 0
    fp32_params = 0
    for k, v in model_state.items():
        if hasattr(v, 'numel'):
            total_params += v.numel()
            if v.dtype == torch.float32:
                fp32_params += v.numel()
    
    print(f"\næ¨¡å‹ä¿¡æ¯:")
    print(f"  æ€»å‚æ•°é‡: {total_params:,}")
    print(f"  FP32 å‚æ•°: {fp32_params:,}")
    print(f"  å…¶ä»–ç±»å‹: {total_params - fp32_params:,}")
    
    # è½¬æ¢ä¸º FP16
    print(f"\nğŸ”„ è½¬æ¢ä¸º FP16...")
    model_state_fp16 = {}
    converted_count = 0
    
    for k, v in model_state.items():
        if hasattr(v, 'dtype') and v.dtype == torch.float32:
            model_state_fp16[k] = v.half()
            converted_count += 1
        else:
            model_state_fp16[k] = v
    
    print(f"âœ“ è½¬æ¢äº† {converted_count} ä¸ª FP32 å‚æ•°")
    
    # ä¿å­˜ FP16 æ¨¡å‹
    print(f"\nğŸ’¾ ä¿å­˜ FP16 æ¨¡å‹åˆ°: {output_path}")
    output_path.parent.mkdir(parents=True, exist_ok=True)
    torch.save(model_state_fp16, output_path)
    
    # æ˜¾ç¤ºå¤§å°å¯¹æ¯”
    original_size = input_path.stat().st_size / (1024**2)
    new_size = output_path.stat().st_size / (1024**2)
    saved = original_size - new_size
    saved_pct = (saved / original_size) * 100
    
    print("\n" + "=" * 70)
    print("âœ¨ é‡åŒ–ç»“æœ")
    print("=" * 70)
    print(f"FP32 æ¨¡å‹:       {original_size:>8.1f} MB")
    print(f"FP16 æ¨¡å‹:       {new_size:>8.1f} MB")
    print(f"èŠ‚çœç©ºé—´:        {saved:>8.1f} MB ({saved_pct:.1f}%)")
    print("=" * 70)
    
    print(f"\nğŸ“ ä½¿ç”¨è¯´æ˜:")
    print(f"   åœ¨æ¨ç†æ—¶åŠ è½½ FP16 æ¨¡å‹åï¼Œéœ€è¦å°†æ¨¡å‹ä¹Ÿè½¬ä¸º FP16:")
    print(f"   ```python")
    print(f"   model.load_state_dict(torch.load('{output_path}'))")
    print(f"   if device.type == 'cuda':")
    print(f"       model = model.half()  # ä½¿ç”¨ FP16 æ¨ç†")
    print(f"   ```")
    print(f"\nâœ“ å®Œæˆï¼FP16 æ¨¡å‹å¯ç”¨äºæ¨ç†ï¼Œç²¾åº¦æŸå¤± <1%")


if __name__ == "__main__":
    quantize_to_fp16(INPUT_PATH, OUTPUT_PATH)

