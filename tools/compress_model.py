#!/usr/bin/env python
"""
ä¸€é”®å‹ç¼©æ¨¡å‹ï¼šæå–æƒé‡ + FP16 é‡åŒ–

å°†è®­ç»ƒçš„ checkpoint ä» ~2.5 GB å‹ç¼©åˆ° ~1.0 GBï¼Œå‡ ä¹æ— ç²¾åº¦æŸå¤±ã€‚

ä½¿ç”¨æ–¹æ³•:
    python tools/compress_model.py
"""

import torch
from pathlib import Path

# ============================================================================
# é…ç½®
# ============================================================================
INPUT_CHECKPOINT = Path(r"D:\qianpf\code\sam3-main\experiments_jiaodai\checkpoints\checkpoint_5.pt")
OUTPUT_FP16 = Path(r"D:\qianpf\code\sam3-main\experiments_jiaodai\checkpoints\checkpoint_1210.pt")
KEEP_INTERMEDIATE = False  # æ˜¯å¦ä¿ç•™ä¸­é—´çš„ FP32 çº¯æƒé‡æ–‡ä»¶


def compress_model(input_path: Path, output_path: Path):
    """ä¸€é”®å‹ç¼©ï¼šæå–æƒé‡ + FP16 é‡åŒ–"""
    print("=" * 70)
    print("SAM3 æ¨¡å‹å‹ç¼©å·¥å…·")
    print("æå–æƒé‡ + FP16 é‡åŒ– = 60% å‹ç¼©ç‡")
    print("=" * 70)
    
    if not input_path.exists():
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
        return
    
    # Step 1: åŠ è½½ checkpoint
    print(f"\nğŸ“‚ æ­¥éª¤ 1/3: åŠ è½½ checkpoint")
    print(f"   æ–‡ä»¶: {input_path}")
    checkpoint = torch.load(input_path, map_location="cpu")
    
    # æå–æ¨¡å‹æƒé‡
    if "model" in checkpoint:
        model_state = checkpoint["model"]
        epoch = checkpoint.get("epoch", "unknown")
        print(f"   âœ“ æå–æ¨¡å‹æƒé‡ (epoch: {epoch})")
    elif "state_dict" in checkpoint:
        model_state = checkpoint["state_dict"]
        print(f"   âœ“ æå–æ¨¡å‹æƒé‡ (state_dict)")
    else:
        model_state = checkpoint
        print(f"   âœ“ ä½¿ç”¨å®Œæ•´ checkpoint")
    
    # Step 2: è½¬æ¢ä¸º FP16
    print(f"\nğŸ”„ æ­¥éª¤ 2/3: è½¬æ¢ä¸º FP16")
    model_state_fp16 = {}
    fp32_count = 0
    total_count = 0
    
    for k, v in model_state.items():
        total_count += 1
        if hasattr(v, 'dtype') and v.dtype == torch.float32:
            model_state_fp16[k] = v.half()
            fp32_count += 1
        else:
            model_state_fp16[k] = v
    
    print(f"   âœ“ è½¬æ¢äº† {fp32_count}/{total_count} ä¸ªå‚æ•°")
    
    # Step 3: ä¿å­˜å‹ç¼©æ¨¡å‹
    print(f"\nğŸ’¾ æ­¥éª¤ 3/3: ä¿å­˜å‹ç¼©æ¨¡å‹")
    print(f"   è¾“å‡º: {output_path}")
    output_path.parent.mkdir(parents=True, exist_ok=True)
    torch.save(model_state_fp16, output_path)
    print(f"   âœ“ ä¿å­˜å®Œæˆ")
    
    # æ˜¾ç¤ºå‹ç¼©ç»“æœ
    original_size = input_path.stat().st_size / (1024**2)
    compressed_size = output_path.stat().st_size / (1024**2)
    saved = original_size - compressed_size
    saved_pct = (saved / original_size) * 100
    
    print("\n" + "=" * 70)
    print("âœ¨ å‹ç¼©å®Œæˆ")
    print("=" * 70)
    print(f"åŸå§‹ checkpoint:  {original_size:>8.1f} MB (100.0%)")
    print(f"å‹ç¼©åæ¨¡å‹:      {compressed_size:>8.1f} MB ({100-saved_pct:.1f}%)")
    print(f"èŠ‚çœç©ºé—´:        {saved:>8.1f} MB ({saved_pct:.1f}%)")
    print("=" * 70)
    
    print(f"\nğŸ“ ä½¿ç”¨è¯´æ˜:")
    print(f"   1. ä¿®æ”¹ batch_inference.py ä¸­çš„é…ç½®:")
    print(f"      CHECKPOINT_PATH = Path('{output_path}')")
    print(f"      USE_FP16 = True")
    print(f"   ")
    print(f"   2. è¿è¡Œæ¨ç†:")
    print(f"      python tools/batch_inference.py")
    
    print(f"\nğŸ’¡ æç¤º:")
    print(f"   - FP16 æ¨¡å‹ç²¾åº¦æŸå¤± <1%")
    print(f"   - GPU æ¨ç†æ—¶å¯èƒ½æ›´å¿«")
    print(f"   - CPU æ¨ç†ä¼šè‡ªåŠ¨è½¬å› FP32")


if __name__ == "__main__":
    compress_model(INPUT_CHECKPOINT, OUTPUT_FP16)

