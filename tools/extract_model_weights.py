#!/usr/bin/env python
"""
ä»è®­ç»ƒçš„ checkpoint ä¸­æå–çº¯æ¨¡å‹æƒé‡ï¼Œå»é™¤ optimizerã€loss ç­‰è®­ç»ƒçŠ¶æ€ã€‚
å¯ä»¥èŠ‚çœ 20-30% çš„å­˜å‚¨ç©ºé—´ã€‚

ä½¿ç”¨æ–¹æ³•:
    python tools/extract_model_weights.py
"""

import torch
from pathlib import Path

# ============================================================================
# é…ç½®
# ============================================================================
CHECKPOINT_PATH = Path(r"D:\qianpf\code\sam3-main\experiments\checkpoints\checkpoint_7.pt")
OUTPUT_PATH = Path(r"D:\qianpf\code\sam3-main\experiments\checkpoints\checkpoint_7_only.pt")


def extract_model_only(checkpoint_path: Path, output_path: Path):
    """ä»å®Œæ•´ checkpoint æå–çº¯æ¨¡å‹æƒé‡"""
    print("=" * 70)
    print("æ¨¡å‹æƒé‡æå–å·¥å…·")
    print("=" * 70)
    
    if not checkpoint_path.exists():
        print(f"âŒ Checkpoint ä¸å­˜åœ¨: {checkpoint_path}")
        return
    
    print(f"\nğŸ“‚ åŠ è½½ checkpoint: {checkpoint_path}")
    checkpoint = torch.load(checkpoint_path, map_location="cpu")
    
    # æå–æ¨¡å‹æƒé‡
    if "model" in checkpoint:
        model_state = checkpoint["model"]
        print(f"âœ“ æ‰¾åˆ°æ¨¡å‹æƒé‡ (epoch: {checkpoint.get('epoch', 'unknown')})")
    elif "state_dict" in checkpoint:
        model_state = checkpoint["state_dict"]
        print(f"âœ“ æ‰¾åˆ°æ¨¡å‹æƒé‡ (state_dict)")
    else:
        model_state = checkpoint
        print(f"âœ“ ä½¿ç”¨æ•´ä¸ª checkpoint ä½œä¸ºæ¨¡å‹æƒé‡")
    
    # æ˜¾ç¤ºåŸå§‹ checkpoint åŒ…å«çš„å†…å®¹
    if isinstance(checkpoint, dict):
        print(f"\nåŸå§‹ checkpoint åŒ…å«:")
        for key in checkpoint.keys():
            if key != "model":
                size = 0
                if isinstance(checkpoint[key], dict):
                    size = sum(v.numel() * v.element_size() 
                             for v in checkpoint[key].values() 
                             if hasattr(v, 'numel'))
                elif hasattr(checkpoint[key], 'numel'):
                    size = checkpoint[key].numel() * checkpoint[key].element_size()
                size_mb = size / (1024**2)
                print(f"  - {key}: {size_mb:.1f} MB")
    
    # ä¿å­˜çº¯æƒé‡
    print(f"\nğŸ’¾ ä¿å­˜çº¯æ¨¡å‹æƒé‡åˆ°: {output_path}")
    output_path.parent.mkdir(parents=True, exist_ok=True)
    torch.save(model_state, output_path)
    
    # æ˜¾ç¤ºå¤§å°å¯¹æ¯”
    original_size = checkpoint_path.stat().st_size / (1024**2)
    new_size = output_path.stat().st_size / (1024**2)
    saved = original_size - new_size
    saved_pct = (saved / original_size) * 100
    
    print("\n" + "=" * 70)
    print("âœ¨ å‹ç¼©ç»“æœ")
    print("=" * 70)
    print(f"åŸå§‹ checkpoint:  {original_size:>8.1f} MB")
    print(f"çº¯æ¨¡å‹æƒé‡:      {new_size:>8.1f} MB")
    print(f"èŠ‚çœç©ºé—´:        {saved:>8.1f} MB ({saved_pct:.1f}%)")
    print("=" * 70)
    print(f"\nâœ“ å®Œæˆï¼å¯ä»¥ä½¿ç”¨ {output_path} è¿›è¡Œæ¨ç†")


if __name__ == "__main__":
    extract_model_only(CHECKPOINT_PATH, OUTPUT_PATH)

